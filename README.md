# Table of Contents

### Table of Contents

- [Solution of Linear Equations](#solution-of-linear-equations)
  - [Gauss Elimination Method](#gauss-elimination-method)
    - [Theory](#gauss-elimination-theory)
      - [Introduction](#gauss-elimination-introduction)
      - [Formula](#gauss-elimination-formula)
      - [Algorithm Steps](#gauss-elimination-algorithm-steps)
      - [Application](#gauss-elimination-application)
    - [Code](#gauss-elimination-code)
    - [Input](#gauss-elimination-input)
    - [Output](#gauss-elimination-output)
  - [Gauss Jordan Elimination Method](#gauss-jordan-elimination-method)
    - [Theory](#gauss-jordan-theory)
      - [Introduction](#gauss-jordan-introduction)
      - [Formula](#gauss-jordan-formula)
      - [Algorithm Steps](#gauss-jordan-algorithm-steps)
      - [Application](#gauss-jordan-application)
    - [Code](#gauss-jordan-code)
    - [Input](#gauss-jordan-input)
    - [Output](#gauss-jordan-output)
  - [LU Decomposition Method](#lu-decomposition-method)
    - [Theory](#gauss-jordan-theory)
      - [Introduction](#gauss-jordan-introduction)
      - [Formula](#gauss-jordan-formula)
      - [Algorithm Steps](#gauss-jordan-algorithm-steps)
      - [Application](#gauss-jordan-application)
    - [Code](#lu-decomposition-code)
    - [Input](#lu-decomposition-input)
    - [Output](#lu-decomposition-output)
  - [Matrix Inversion](#matrix-inversion)
    - [Theory](#matrix-inversion-theory)
    - [Code](#matrix-inversion-code)
    - [Input](#matrix-inversion-input)
    - [Output](#matrix-inversion-output)

- [Solution of Non-Linear Equations](#solution-of-non-linear-equations)
  - [Bisection Method](#bisection-method)
    - [Theory](#bisection-theory)
    - [Code](#bisection-code)
    - [Input](#bisection-input)
    - [Output](#bisection-output)
  - [False Position Method](#false-position-method)
    - [Theory](#false-position-theory)
    - [Code](#false-position-code)
    - [Input](#false-position-input)
    - [Output](#false-position-output)
  - [Secant Method](#secant-method)
    - [Theory](#secant-theory)
    - [Code](#secant-code)
    - [Input](#secant-input)
    - [Output](#secant-output)
  - [Newton Raphson Method](#newton-raphson-method)
    - [Theory](#newton-raphson-theory)
    - [Code](#newton-raphson-code)
    - [Input](#newton-raphson-input)
    - [Output](#newton-raphson-output)

- [Solution of Interpolation](#solution-of-interpolation)
  - [Newton's Forward Interpolation Method](#newtons-forward-interpolation-method)
    - [Theory](#newtons-forward-interpolation-theory)
      - [Introduction](#newtons-forward-interpolation-introduction)
      - [Formula](#newtons-forward-interpolation-formula)
      - [Algorithm Steps](#newtons-forward-interpolation-algorithm-steps)
      - [Application](#newtons-forward-interpolation-application)
    - [Code](#newtons-forward-interpolation-code)
    - [Input](#newtons-forward-interpolation-input)
    - [Output](#newtons-forward-interpolation-output)
  - [Newton's Backward Interpolation Method](#newtons-backward-interpolation-method)
    - [Theory](#newtons-backward-interpolation-theory)
      - [Introduction](#newtons-backward-interpolation-introduction)
      - [Formula](#newtons-backward-interpolation-formula)
      - [Algorithm Steps](#newtons-backward-interpolation-algorithm-steps)
      - [Application](#newtons-backward-interpolation-application)
    - [Code](#newtons-backward-interpolation-code)
    - [Input](#newtons-backward-interpolation-input)
    - [Output](#newtons-backward-interpolation-output)
  - [Divided Difference Method](#divided-difference-method)
    - [Theory](#divided-difference-theory)
      - [Introduction](#divided-difference-introduction)
      - [Formula](#divided-difference-formula)
      - [Algorithm Steps](#divided-difference-steps)
      - [Application](#divided-difference-application)
    - [Code](#divided-difference-code)
    - [Input](#divided-difference-input)
    - [Output](#divided-difference-output)

- [Solution of Curve Fitting Model](#solution-of-curve-fitting-model)
  - [Least Square Regression Method For Linear Equations](#least-square-regression-method-for-linear-equations)
    - [Theory](#least-square-regression-method-for-linear-equations-theory)
      - [Introduction](#least-square-regression-method-for-linear-equations-introduction)
      - [Formula](#least-square-regression-method-for-linear-equations-formula)
      - [Algorithm Steps](#least-square-regression-method-for-linear-equations-steps)
      - [Application](#least-square-regression-method-for-linear-equations-application)
    - [Code](#least-square-regression-method-for-linear-equations-code)
    - [Input](#least-square-regression-method-for-linear-equations-input)
    - [Output](#least-square-regression-method-for-linear-equations-output)
  - [Least Square Regression Method For Transcendental Equations](#least-square-regression-method-for-transcendental-equations)
    - [Theory](#least-square-regression-method-for-transcendental-equations-theory)
      - [Introduction](#least-square-regression-method-for-transcendental-equations-introduction)
      - [Formula](#least-square-regression-method-for-transcendental-equations-formula)
      - [Algorithm Steps](#least-square-regression-method-for-transcendental-equations-steps)
      - [Application](#least-square-regression-method-for-transcendental-equations-application)
    - [Code](#least-square-regression-method-for-transcendental-equations-code)
    - [Input](#least-square-regression-method-for-transcendental-equations-input)
    - [Output](#least-square-regression-method-for-transcendental-equations-output)
  - [Least Square Regression Method For Polynomial Equations](#least-square-regression-method-for-polynomial-equations)
    - [Theory](#least-square-regression-method-for-polynomial-equations-theory)
      - [Introduction](#least-square-regression-method-for-polynomial-equations-introduction)
      - [Formula](#least-square-regression-method-for-polynomial-equations-formula)
      - [Algorithm Steps](#least-square-regression-method-for-polynomial-equations-steps)
      - [Application](#least-square-regression-method-for-polynomial-equations-application)
    - [Code](#least-square-regression-method-for-polynomial-equations-code)
    - [Input](#least-square-regression-method-for-polynomial-equations-input)
    - [Output](#least-square-regression-method-for-polynomial-equations-output)

- [Solution of Differential Equations](#solution-of-differential-equations)
  - [Equal Interval Interpolation Method](#equal-interval-interpolation-method)
    - [Theory](#equal-interval-interpolation-theory)
    - [Code](#equal-interval-interpolation-code)
    - [Input](#equal-interval-interpolation-input)
    - [Output](#equal-interval-interpolation-output)
  - [Second Order Derivative Method](#second-order-derivative-method)
    - [Theory](#second-order-derivative-theory)
    - [Code](#second-order-derivative-code)
    - [Input](#second-order-derivative-input)
    - [Output](#second-order-derivative-output)
  - [Runge Kutta Method](#runge-kutta-method)
    - [Theory](#runge-kutta-theory)
    - [Code](#runge-kutta-code)
    - [Input](#runge-kutta-input)
    - [Output](#runge-kutta-output)
  - [Numerical Differentiation Method](#numerical-differentiation-method)
    - [Theory](#numerical-differentiation-theory)
    - [Code](#numerical-differentiation-code)
    - [Input](#numerical-differentiation-input)
    - [Output](#numerical-differentiation-output)

- [Solution of Numerical Integrations](#solution-of-numerical-integrations)
  - [Simpson's One-Third Rule](#simpsons-one-third-rule)
    - [Theory](#simpsons-one-third-rule-theory)
    - [Code](#simpsons-one-third-rule-code)
    - [Input](#simpsons-one-third-rule-input)
    - [Output](#simpsons-one-third-rule-output)
  - [Simpson's Three-Eighths Rule](#simpsons-three-eighths-rule)
    - [Theory](#simpsons-three-eighths-rule-theory)
    - [Code](#simpsons-three-eighths-rule-code)
    - [Input](#simpsons-three-eighths-rule-input)
    - [Output](#simpsons-three-eighths-rule-output)


---

### Solution of Linear Equations

### Gauss Elimination Method

#### Gauss Elimination Theory
this is gauss eli

#### Gauss Elimination Code
-[code](Solution Of Linear equations\Gauss Elimination\)

#### Gauss Elimination Input
```
[Add your input format here]
```

#### Gauss Elimination Output
```
[Add your output format here]
```

---

### Gauss Jordan Elimination Method

#### Gauss Jordan Theory
# Polynomial Regression

## Polynomial Regression – Introduction
Polynomial regression is a statistical method used to fit a higher-degree polynomial to a set of data points. Unlike linear regression, which fits a straight line, polynomial regression can capture curvature in the data.  

The assumed form of the polynomial is:

y = a0 + a1*x + a2*x^2 + a3*x^3 + ... + an*x^n

Where:  
- a0, a1, a2, ..., an are the coefficients of the polynomial  
- n is the degree of the polynomial  

The coefficients are determined such that the sum of the squares of the differences between the observed values and the predicted values is minimized.

---

## Polynomial Regression – Formula / Concept
For a polynomial of degree n, the **normal equations** are:

Σy = n*a0 + a1*Σx + a2*Σx^2 + ... + an*Σx^n

Σ(x*y) = a0*Σx + a1*Σx^2 + a2*Σx^3 + ... + an*Σx^(n+1)

Σ(x^2*y) = a0*Σx^2 + a1*Σx^3 + a2*Σx^4 + ... + an*Σx^(n+2)

...

Σ(x^n*y) = a0*Σx^n + a1*Σx^(n+1) + a2*Σx^(n+2) + ... + an*Σx^(2n)

Solve these equations simultaneously to determine the coefficients a0, a1, ..., an.

---

## Polynomial Regression – Procedure
1. Collect the data points (x, y).  
2. Decide the degree n of the polynomial.  
3. Compute the required sums: Σx, Σx^2, ..., Σx^(2n), Σy, Σ(x*y), Σ(x^2*y), ..., Σ(x^n*y).  
4. Form the normal equations:

Σy = n*a0 + a1*Σx + a2*Σx^2 + ... + an*Σx^n  
Σ(x*y) = a0*Σx + a1*Σx^2 + a2*Σx^3 + ... + an*Σx^(n+1)  
Σ(x^2*y) = a0*Σx^2 + a1*Σx^3 + a2*Σx^4 + ... + an*Σx^(n+2)  
...  
Σ(x^n*y) = a0*Σx^n + a1*Σx^(n+1) + a2*Σx^(n+2) + ... + an*Σx^(2n)  

5. Solve the normal equations simultaneously to find the coefficients a0, a1, ..., an.  
6. Construct the regression polynomial:

y = a0 + a1*x + a2*x^2 + ... + an*x^n  

7. Use the polynomial to predict y for any given x.


#### Gauss Jordan Code
```python
# Add your code here
```

#### Gauss Jordan Input
```
[Add your input format here]
```

#### Gauss Jordan Output
```
[Add your output format here]
```

---

### LU Decomposition Method

#### LU Decomposition Theory
[Add your theory content here]

#### LU Decomposition Code
```python
# Add your code here
```

#### LU Decomposition Input
```
[Add your input format here]
```

#### LU Decomposition Output
```
[Add your output format here]
```

---

### Matrix Inversion

#### Matrix Inversion Theory
[Add your theory content here]

#### Matrix Inversion Code
```python
# Add your code here
```

#### Matrix Inversion Input
```
[Add your input format here]
```

#### Matrix Inversion Output
```
[Add your output format here]
```

---

### Solution of Non-Linear Equations

### Bisection Method

#### Bisection Theory
[Add your theory content here]

#### Bisection Code
```python
# Add your code here
```

#### Bisection Input
```
[Add your input format here]
```

#### Bisection Output
```
[Add your output format here]
```

---

### False Position Method

#### False Position Theory
[Add your theory content here]

#### False Position Code
```python
# Add your code here
```

#### False Position Input
```
[Add your input format here]
```

#### False Position Output
```
[Add your output format here]
```

---






### Solution of Interpolation

### Newton's Forward Interpolation Method

#### Newton's Forward Interpolation Theory
##### Newton's Forward Interpolation Introduction

Newton’s Forward Interpolation method is a numerical technique used to estimate the value of a function at a point when the independent variable values are equally spaced. It is particularly effective when the required value lies near the beginning of the data table.
The method uses forward differences of the function values to construct the interpolation polynomial.


##### Newton's Forward Interpolation Formula
Interpolation formula:

y = y0 + p*Δy0 + (p*(p-1)/2!)*Δ²y0 + (p*(p-1)*(p-2)/3!)*Δ³y0 + ...

where:

p = (x - x0) / h

Explanation of terms:

- y0   : The value of the function at the first data point.
- Δy0, Δ²y0, Δ³y0, ... : Forward differences of the function values.
- h    : Spacing between consecutive values of x.
- x    : The point at which interpolation is required.
- p    : Ratio of distance from the first data point in units of h.

This formula estimates y at x by successively adding terms involving forward differences,
where each term accounts for higher-order variations of the data.



##### Newton's Forward Interpolation Algorithm Steps

1. Arrange the data:
   Ensure that the independent variable values x0, x1, ..., xn are equally spaced.

2. Construct a forward difference table:
   Compute the forward differences Δy0, Δ²y0, Δ³y0, ... from the given data.

3. Calculate p:
   p = (x - x0) / h
   where x is the value at which interpolation is required
   and h is the spacing between consecutive x values.

4. Apply the interpolation formula:
   y = y0 + p*Δy0 + (p*(p-1)/2!)*Δ²y0 + (p*(p-1)*(p-2)/3!)*Δ³y0 + ...
   Substitute y0, Δy0, Δ²y0, ... and p to compute the interpolated value of y.

5. Evaluate higher-order terms if necessary:
   Include as many terms as needed for the desired accuracy.



##### Newton's Forward Interpolation Application
```
[Add your output format here]
```

#### Newton's Forward Interpolation Code

```python
# Add your code here
```
#### Newton's Forward Interpolation Input
```
[Add your output format here]
```
#### Newton's Forward Interpolation Output
```
[Add your output format here]
```




### Newton's Backward Interpolation Method

#### Newton's Backward Interpolation Theory
##### Newton's Backward Interpolation Introduction

Newton’s Backward Interpolation method is used when the data points are equally spaced
and the required value lies near the end of the data table.
This method uses backward differences to estimate the value of the function.



##### Newton's Backward Interpolation Formula
```
y = yn + p*∇y_n + (p*(p+1)/2!)*∇²y_n + (p*(p+1)*(p+2)/3!)*∇³y_n + ...

where:

p = (x - xn) / h

Explanation of terms:

- yn    : The value of the function at the last data point.
- ∇y_n, ∇²y_n, ∇³y_n, ... : Backward differences of the function values.
- h     : Spacing between consecutive values of x.
- x     : The point at which interpolation is required.
- p     : Ratio of distance from the last data point in units of h.

```

##### Newton's Backward Interpolation Algorithm Steps
1. Arrange the data:
   Ensure that the independent variable values x0, x1, ..., xn are equally spaced.

2. Construct a backward difference table:
   Compute the backward differences ∇y_n, ∇²y_n, ∇³y_n, ... from the given data.

3. Calculate p:
   p = (x - xn) / h
   where x is the value at which interpolation is required
   and h is the spacing between consecutive x values.

4. Apply the interpolation formula:
   y = yn + p*∇y_n + (p*(p+1)/2!)*∇²y_n + (p*(p+1)*(p+2)/3!)*∇³y_n + ...
   Substitute yn, ∇y_n, ∇²y_n, ... and p to compute the interpolated value of y.

5. Evaluate higher-order terms if necessary:
   Include as many terms as needed for the desired accuracy.


##### Newton's Backward Interpolation Application
```
[Add your output format here]
```

#### Newton's Backward Interpolation Code
```python
# Add your code here
```

#### Newton's Backward Interpolation Input
```
[Add your output format here]
```
#### Newton's Backward Interpolation Output
```
[Add your output format here]
```




### divided difference  Method

#### divided difference Theory
##### divided difference Introduction
Newton’s Divided Difference Interpolation is used when the data points are unequally spaced.
It constructs an interpolation polynomial that passes through all the given data points.
This method uses divided differences to compute the coefficients of the polynomial.


##### divided difference Formula
```
P(x) = y0 
       + (x - x0) * f[x0, x1] 
       + (x - x0)*(x - x1) * f[x0, x1, x2] 
       + (x - x0)*(x - x1)*(x - x2) * f[x0, x1, x2, x3] 
       + ...

where:

- y0                  : The value of the function at the first data point.
- f[x0, x1], f[x0,x1,x2], ... : First, second, and higher-order divided differences.
- x                    : The point at which interpolation is required.

```

##### divided difference Steps

1. Arrange the data:
   List the data points (x0, y0), (x1, y1), ..., (xn, yn).

2. Construct a divided difference table:
   - Compute first-order divided differences: f[xi, xi+1] = (yi+1 - yi) / (xi+1 - xi)
   - Compute second-order divided differences: f[xi, xi+1, xi+2] = (f[xi+1, xi+2] - f[xi, xi+1]) / (xi+2 - xi)
   - Continue calculating higher-order divided differences as needed.

3. Form the interpolation polynomial:
   P(x) = y0 + (x - x0)*f[x0,x1] + (x - x0)*(x - x1)*f[x0,x1,x2] + ...

4. Evaluate P(x) at the required value of x:
   Substitute the computed divided differences and the value of x to get the interpolated value of y.



##### divided difference Application
```
[Add your output format here]
```

#### divided difference Code
```python
# Add your code here
```

#### divided difference Input
```
[Add your output format here]
```
#### divided difference Output
```
[Add your output format here]
```



### solution-of-curve-fitting-model

### least square regression method for linear equations
#### least square regression method for linear equations theory

##### least square regression method for linear equations – Introduction


##### least square regression method for linear equations – Formula 

##### least square regression method for linear equations – steps

##### least-square-regression-method-for-linear-equations-application

#### lleast square regression method for linear equations Code
```python
# Add your code here
```

#### least square regression method for linear equations Input
```
[Add your output format here]
```
#### least square regression method for linear equations Output
```
[Add your output format here]
```




















# least square regression method for Polynomial equations
# least square regression method for Polynomial equations theory

## least square regression method for Polynomial equations – Introduction
Polynomial regression is a statistical method used to fit a higher-degree polynomial to a set of data points. Unlike linear regression, which fits a straight line, polynomial regression can capture curvature in the data.  

The assumed form of the polynomial is:

y = a0 + a1*x + a2*x^2 + a3*x^3 + ... + an*x^n

Where:  
- a0, a1, a2, ..., an are the coefficients of the polynomial  
- n is the degree of the polynomial  

The coefficients are determined such that the sum of the squares of the differences between the observed values and the predicted values is minimized.

---

## least square regression method for Polynomial equations – Formula 
For a polynomial of degree n, the **normal equations** are:

Σy = n*a0 + a1*Σx + a2*Σx^2 + ... + an*Σx^n

Σ(x*y) = a0*Σx + a1*Σx^2 + a2*Σx^3 + ... + an*Σx^(n+1)

Σ(x^2*y) = a0*Σx^2 + a1*Σx^3 + a2*Σx^4 + ... + an*Σx^(n+2)

...

Σ(x^n*y) = a0*Σx^n + a1*Σx^(n+1) + a2*Σx^(n+2) + ... + an*Σx^(2n)

Solve these equations simultaneously to determine the coefficients a0, a1, ..., an.

---

## least square regression method for Polynomial equations – steps
1. Collect the data points (x, y).  
2. Decide the degree n of the polynomial.  
3. Compute the required sums: Σx, Σx^2, ..., Σx^(2n), Σy, Σ(x*y), Σ(x^2*y), ..., Σ(x^n*y).  
4. Form the normal equations:

Σy = n*a0 + a1*Σx + a2*Σx^2 + ... + an*Σx^n  
Σ(x*y) = a0*Σx + a1*Σx^2 + a2*Σx^3 + ... + an*Σx^(n+1)  
Σ(x^2*y) = a0*Σx^2 + a1*Σx^3 + a2*Σx^4 + ... + an*Σx^(n+2)  
...  
Σ(x^n*y) = a0*Σx^n + a1*Σx^(n+1) + a2*Σx^(n+2) + ... + an*Σx^(2n)  

5. Solve the normal equations simultaneously to find the coefficients a0, a1, ..., an.  
6. Construct the regression polynomial:

y = a0 + a1*x + a2*x^2 + ... + an*x^n  

7. Use the polynomial to predict y for any given x.
# least-square-regression-method-for-polynomial-equations-application

 #### lleast square regression method for Polynomial equations Code
```python
# Add your code here
```

#### least square regression method for Polynomial equations Input
```
[Add your output format here]
```
#### least square regression method for Polynomial equations Output
```
[Add your output format here]
```
